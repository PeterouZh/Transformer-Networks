# Transformer-Networks

PyTorch implementation of transformer networks

|  Title  |   Venue  |Code|Year|
|:--------|:--------:|:--------:|:--------:|
| [Going Deeper with Image Transformers](http://arxiv.org/abs/2103.17239) | arXiv:2103.17239 [cs] |  | 2021 |
| [Transformer Interpretability Beyond Attention Visualization](http://arxiv.org/abs/2012.09838) | arXiv:2012.09838 [cs] | [github](https://github.com/hila-chefer/Transformer-Explainability) | 2020 |
| [Rethinking Spatial Dimensions of Vision Transformers](http://arxiv.org/abs/2103.16302) | arXiv:2103.16302 [cs] | [github](https://github.com/naver-ai/pit) | 2021 |
| [Scaling Local Self-Attention For Parameter Efficient Visual Backbones](http://arxiv.org/abs/2103.12731) | arXiv:2103.12731 [cs] |  | 2021 |
| [Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows](http://arxiv.org/abs/2103.14030) | arXiv:2103.14030 [cs] | [github](https://github.com/microsoft/Swin-Transformer) | 2021 |
| [Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](http://arxiv.org/abs/2102.12122) | arXiv:2102.12122 [cs] | [github](https://github.com/whai362/PVT), [PVT_Small](exp/PVT/pvt.py) | 2021 |
