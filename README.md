# Transformer-Networks

PyTorch implementation of transformer networks

## Transformer
|  Title  |   Venue  |Code|Year|
|:--------|:--------:|:--------:|:--------:|
| [Visformer: The Vision-Friendly Transformer](http://arxiv.org/abs/2104.12533) | arXiv:2104.12533 [cs] |  | 2021 |
| [ImageNet-21K Pretraining for the Masses](http://arxiv.org/abs/2104.10972) | arXiv:2104.10972 [cs] |  | 2021 |
| [So-ViT: Mind Visual Tokens for Vision Transformer](http://arxiv.org/abs/2104.10935) | arXiv:2104.10935 [cs] |  | 2021 |
| [Token Labeling: Training a 85.4% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNet](http://arxiv.org/abs/2104.10858) | arXiv:2104.10858 [cs] |  | 2021 |
| [Going Deeper with Image Transformers](http://arxiv.org/abs/2103.17239) | arXiv:2103.17239 [cs] |  | 2021 |
| [Transformer Interpretability Beyond Attention Visualization](http://arxiv.org/abs/2012.09838) | arXiv:2012.09838 [cs] | [github](https://github.com/hila-chefer/Transformer-Explainability) | 2020 |
| [Rethinking Spatial Dimensions of Vision Transformers](http://arxiv.org/abs/2103.16302) | arXiv:2103.16302 [cs] | [github](https://github.com/naver-ai/pit) | 2021 |
| [Stand-Alone Self-Attention in Vision Models](http://arxiv.org/abs/1906.05909) | NeurIPS | [github](https://github.com/leaderj1001/Stand-Alone-Self-Attention) | 2019 |
| [Scaling Local Self-Attention For Parameter Efficient Visual Backbones](http://arxiv.org/abs/2103.12731) | arXiv:2103.12731 [cs] |  | 2021 |
| [Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows](http://arxiv.org/abs/2103.14030) | arXiv:2103.14030 [cs] | [github](https://github.com/microsoft/Swin-Transformer) | 2021 |
| [Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](http://arxiv.org/abs/2102.12122) | arXiv:2102.12122 [cs] | [github](https://github.com/whai362/PVT), [PVT_Small](exp/PVT/pvt.py) | 2021 |


## CNN
|  Title  |   Venue  |Code|Year|
|:--------|:--------:|:--------:|:--------:|
| [CondenseNet V2: Sparse Feature Reactivation for Deep Networks](http://arxiv.org/abs/2104.04382) | arXiv:2104.04382 [cs] |  | 2021 |
